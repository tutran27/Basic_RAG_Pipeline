{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aRdt9VYPk8Iv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52fa48b6-cee5-484b-d447-8f7cb815db31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/484.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain_core -q\n",
        "!pip install langchain_community -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model_name='tutran27/Chat_bot_Qwen3-4B'\n",
        "\n",
        "model=AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                           trust_remote_code=True,\n",
        "                                           device_map=device,\n",
        "                                           dtype=torch.float16)\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "df8_XYGKlOoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "\n",
        "pipe_model=pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task='text-generation',\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        ")\n",
        "\n",
        "llms=HuggingFacePipeline(pipeline=pipe_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7RHcGjAlt6Y",
        "outputId": "816205f6-582a-4b10-d1ba-835345c4541c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "/tmp/ipython-input-2626427090.py:16: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFacePipeline``.\n",
            "  llms=HuggingFacePipeline(pipeline=pipe_model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input=\"\"\" <|im_start|> Viáº¿t 1 Ä‘oáº¡n ngáº¯n sÃ¡nh chÃ³ vÃ  mÃ¨o <|im_end|>\n",
        "          <|im_start|>\"\"\"\n",
        "\n",
        "print(llms.invoke(input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoKoRDu7m_ea",
        "outputId": "1d78c950-067f-45e8-d358-0d1864a4460d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ChÃ³ vÃ  mÃ¨o lÃ  hai loÃ i Ä‘á»™ng váº­t cÃ³ vÃº Ä‘Æ°á»£c coi lÃ  báº¡n Ä‘á»“ng hÃ nh trong nhiá»u tháº¿ ká»·. DÃ¹ chÃºng cÃ³ nhá»¯ng Ä‘áº·c Ä‘iá»ƒm khÃ¡c nhau nhÆ°ng cáº£ hai Ä‘á»u cÃ³ má»™t sá»‘ Ä‘iá»ƒm chung. Cáº£ chÃ³ vÃ  mÃ¨o Ä‘á»u Ä‘Æ°á»£c coi lÃ  thÃº cÆ°ng vÃ  chÃºng thÆ°á»ng gáº¯n bÃ³ cháº·t cháº½ vá»›i con ngÆ°á»i. ChÃºng cÅ©ng cÃ³ thá»ƒ ráº¥t thÃ´ng minh, trung thÃ nh vÃ  dá»… nuÃ´i. Máº·c dÃ¹ chÃºng cÃ³ cÃ¡ch tiáº¿p cáº­n cuá»™c sá»‘ng khÃ¡c nhau nhÆ°ng cáº£ hai Ä‘á»u cÃ³ thá»ƒ trá»Ÿ thÃ nh thÃ nh viÃªn gia Ä‘Ã¬nh tuyá»‡t vá»i cá»§a báº¥t ká»³ ai cÃ³ lÃ²ng tá»‘t Ä‘á»‘i xá»­ vá»›i chÃºng.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load document"
      ],
      "metadata": {
        "id": "u992L5dIrmY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw5ir8GcqCAe",
        "outputId": "fd863452-9665-4552-f29c-44a79d50feb7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/329.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "data=PyPDFLoader(\"/content/LÃ½ thuyáº¿t.pdf\")\n",
        "document=data.load()"
      ],
      "metadata": {
        "id": "dLvSA8rGndZI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(document[0])"
      ],
      "metadata": {
        "id": "hF_m78jKpy5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splittext"
      ],
      "metadata": {
        "id": "JzhhwhuarqvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "split_text=RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=40,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "texts=split_text.split_documents(document)"
      ],
      "metadata": {
        "id": "RrKETEs1qlwg"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in texts:\n",
        "  print(x)\n",
        "  break"
      ],
      "metadata": {
        "id": "AjCsWitqrFuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding"
      ],
      "metadata": {
        "id": "dqLnsOYSwJU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_name='Qwen/Qwen3-Embedding-0.6B'\n",
        "embedding_function=HuggingFaceEmbeddings(model_name=embedding_name)"
      ],
      "metadata": {
        "id": "gIxSlAWOrQK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Database"
      ],
      "metadata": {
        "id": "i2xwEDXqtq41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb -q"
      ],
      "metadata": {
        "id": "wE5saBwbtokN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "db=Chroma.from_documents(texts, embedding_function)"
      ],
      "metadata": {
        "id": "4uUoRawqsLLd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever"
      ],
      "metadata": {
        "id": "iPoqNMRRwwwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 2}\n",
        ")\n",
        "\n",
        "query=\"SÃ³ng dá»«ng lÃ  gÃ¬\"\n",
        "output=retriever.invoke(query)\n",
        "print(output[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QVgSZz-sSsg",
        "outputId": "f0587aab-fb44-4684-b968-d9fae96fe4af"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Î© â‰ˆ ğœ”0 \n",
            " \n",
            "(vá»›i ğœ”0lÃ  táº§n sá»‘ gÃ³c riÃªng cá»§a há»‡; ma sÃ¡t cÃ ng nhá» â†’ cá»™ng hÆ°á»Ÿng cÃ ng rÃµ) \n",
            " \n",
            "CHÆ¯Æ NG II: SÃ“NG \n",
            "1) MÃ´ táº£ sÃ³ng cÆ¡ \n",
            "â€¢ SÃ³ng cÆ¡: sá»± lan truyá»n dao Ä‘á»™ng cÆ¡ trong mÃ´i trÆ°á»ng váº­t cháº¥t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "dggAq1xlu5i_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "str_parser=StrOutputParser()\n",
        "\n",
        "template=\"\"\" <|im_start|> Báº¡n lÃ  má»™t chatbot truy xuáº¥t thÃ´ng tin ráº¥t há»¯u Ã­ch. Tá»« thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p hÃ£y hoÃ n thiá»‡n cÃ¢u tráº£ lá»i má»™t cÃ¡ch chÃ­nh xÃ¡c. <|im_end|>\n",
        "<|im_start|> {context} <|im_end|>\n",
        "<|im_start|> CÃ¢u há»i: {question} <|im_end|>\n",
        "<|im_start|>\"\"\"\n",
        "\n",
        "prompt=PromptTemplate( template=template)\n"
      ],
      "metadata": {
        "id": "D-I5MXDXvCBu"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llms\n",
        "    | str_parser\n",
        ")"
      ],
      "metadata": {
        "id": "yYeWvrYQuP5g"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=rag_chain.invoke(\"Giáº£i thÃ­ch sÃ³ng ngang\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3itzvzauvrYe",
        "outputId": "ee7f9819-ee3d-40a0-87cd-cea6692e8086"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Wavengang lÃ  nhá»¯ng sÃ³ng cÃ³ phÆ°Æ¡ng dao Ä‘á»™ng vuÃ´ng gÃ³c vá»›i hÆ°á»›ng chuyá»ƒn Ä‘á»™ng cá»§a chÃºng. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  khi cÃ¡c háº¡t trong váº­t cháº¥t dao Ä‘á»™ng theo phÆ°Æ¡ng tháº³ng Ä‘á»©ng hoáº·c náº±m ngang, chÃºng chuyá»ƒn Ä‘á»™ng theo chiá»u ngang hay dá»c so vá»›i hÆ°á»›ng truyá»n nÄƒng lÆ°á»£ng. VÃ­ dá»¥ vá» sÃ³ng ngang bao gá»“m sÃ³ng trÃªn máº·t nÆ°á»›c vÃ  sÃ³ng cÆ¡ há»c trong cÃ¡c sá»£i dÃ¢y nhÆ° dÃ¢y Ä‘Ã n gÃµ. Trong sÃ³ng ngang, cÃ¡c háº¡t váº­t cháº¥t di chuyá»ƒn theo hÆ°á»›ng vuÃ´ng gÃ³c vá»›i hÆ°á»›ng truyá»n nÄƒng lÆ°á»£ng. CÃ¡c sÃ³ng ngang cÅ©ng Ä‘Æ°á»£c gá»i lÃ  sÃ³ng transverse vÃ¬ chÃºng liÃªn quan Ä‘áº¿n chuyá»ƒn Ä‘á»™ng cá»§a cÃ¡c háº¡t váº­t cháº¥t trong váº­t liá»‡u mÃ  chÃºng Ä‘ang truyá»n nÄƒng lÆ°á»£ng. Nguá»“n phÃ¡t ra sÃ³ng ngang cÃ³ thá»ƒ lÃ  báº¥t ká»³ nguá»“n nÃ o táº¡o ra dao Ä‘á»™ng trÃªn bá» máº·t cá»§a má»™t váº­t liá»‡u nháº¥t Ä‘á»‹nh, cháº³ng háº¡n nhÆ° má»™t dÃ²ng cháº£y trong nÆ°á»›c hoáº·c sá»± rung Ä‘á»™ng cá»§a dÃ¢y. NhÃ¬n chung, sÃ³ng ngang lÃ  loáº¡i sÃ³ng phá»• biáº¿n nháº¥t Ä‘Æ°á»£c tÃ¬m tháº¥y trong mÃ´i trÆ°á»ng tá»± nhiÃªn, bao gá»“m cáº£ cÃ¡c hiá»‡n tÆ°á»£ng tá»± nhiÃªn nhÆ° sÃ³ng biá»ƒn vÃ  sÃ³ng trÃªn bÄƒng biá»ƒn. ChÃºng cÅ©ng thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c á»©ng dá»¥ng cÃ´ng nghiá»‡p Ä‘á»ƒ truyá»n táº£i tÃ­n hiá»‡u vÃ  dá»¯ liá»‡u qua cÃ¡c sá»£i cÃ¡p vÃ  dÃ¢y Ä‘iá»‡n thoáº¡i.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MyxgFLPuwCFQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}